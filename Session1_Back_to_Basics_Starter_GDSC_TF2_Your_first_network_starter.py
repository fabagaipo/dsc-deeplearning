# -*- coding: utf-8 -*-
"""Copy of Back to Basics Starter GDSC TF2- Your first network - starter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GveipTpeVFfMM6TAJuZEjBWyueGe9ILN

### First we need to download the dependency files 

This will download, unzip then clean up
"""

# Setup, download, unzip
!wget -qq https://www.dropbox.com/s/4wdgtnr4z950hcf/notebook1.zip 
!unzip -qq notebook1.zip

!rm -r __MACOSX

ls

"""# Your first tf.keras Network

4 Building Blocks:

- Data for inputs and labels
- Architecture
- Loss
- Optimizer


"""

#imports 
import matplotlib.pyplot as plt
import numpy as np

import tensorflow as tf

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input
from tensorflow.keras.optimizers import SGD

from tensorflow.keras.utils import plot_model

#versions
print(tf.__version__)
print(tf.keras.__version__)

"""### Lets import our dataset"""

# import MNIST
from tensorflow.keras.datasets import mnist

(x_train,y_train),(x_test,y_test) = mnist.load_data()

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

#examine what the first image looks like
x_train[0]

# the first label
y_train[0]

from IPython.display import Image

"""## What is a Tensor


multi dimensional array/matrix
"""

Image('./diagram5g.png', width=800)

"""### MNIST Image Tensor X_train


"""

Image('./diagram4f.png', width=550)

Image('./diagram6d.png', width=550)

"""## Preparing the data for a basic network"""

# changing the shape of our data to be flat vectors instead of matrices
x_train = x_train.reshape(60000, 784)
x_test = x_test.reshape(10000, 784)

# Converting the ints to float for floating point math
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

# Standardizing the data to be between 0-1 --- Be careful to only do this once
x_train /=255.0
x_test /=255.0


print(x_train.shape, 'train samples')
print(x_test.shape, 'test samples')

#examine what the first image looks like
x_train[0]

# first label
y_train[0]

"""### Making One Hot Encoded vectors"""

# number of classes
n_classes = 10

# Convert Class Scalars to  One Hot Encoded vectors
y_train = tf.keras.utils.to_categorical(y_train, n_classes)
y_test = tf.keras.utils.to_categorical(y_test, n_classes)

# look at our label now
y_train[0]

# lets check our tensor shape
y_train.shape

"""## Plotting some examples"""

# Method for displaying the number as a picture

def show_digit(index):
    label = y_train[index].argmax(axis=0)
    # Reshape 784 array into 28x28 image
    image = x_train[index].reshape([28,28])
    fig, axes = plt.subplots(1, )
    fig.subplots_adjust(hspace=0.5, wspace=0.5)
    plt.title('Training data, index: %d,  Label: %d' % (index, label))
    plt.imshow(image, cmap='gray_r')
    plt.show()
    
def show_predicted_digit(image, pred, label):
    # Reshape 784 array into 28x28 image
    image = image.reshape([28,28])
    plt.title('Original Image, Pred: %d,  True Label: %d' %(pred, label))
    plt.imshow(image, cmap='gray_r')
    plt.show()
    
# Display the first (index 0) training image
show_digit(0)
show_digit(2)
show_digit(3)

"""### Setting up our Network Hyper Parameters"""

# Training Parameters for basic MNIST
learning_rate = 0.001
training_epochs = 10
batch_size = 100

# Network Parameters
n_input = 784 ## MNIST data input (img shape: 28*28 flattened to be 784)
n_hidden_1 = 392   # 1st layer number of neurons
n_hidden_2 = 100 # 2nd layer number of neurons
n_classes = 10 # MNIST classes for prediction(digits 0-9 )

"""## Lets build our first network


"""

# building the model
Inp = Input(shape=(784,), name='Input')
x = Dense(n_hidden_1, activation='relu', name='Dense_01')(Inp)
x = Dense(n_hidden_2, activation='relu',name='Dense_02')(x)
# x = Dense(n_hidden_2, activation='relu',name='Dense_03')(x)
# x = Dense(50, activation='relu',name='Dense_04')(x)
outputs = Dense(n_classes,activation='softmax',name='Outputs')(x)

model = Model(Inp, outputs, name='basic_mnist_model')

# lets look at the model
model.summary()

"""### Lets calculate the the parameters

layer 1 = (392 units * 784 weights) + (392 units * 1 bias)  = 307,720  

layer 2 = (100 units * 392 weights) + (100 units * 1 bias)  = 39,300  

layer 3 = (10 units * 100) +(10 units * bias)



"""

# plot the model out
plot_model(model, to_file="first_model.png")

"""## Create the optimizer and compile the graph"""

# setting up our Optimizer
opt = SGD(learning_rate=learning_rate)

# compile the model
model.compile(loss='categorical_crossentropy', 
              optimizer=opt, 
              metrics=['accuracy'])

"""## Training time

we fit the model using the data we pass in to it 

AKA training the model
"""

# train the model
model.fit(x_train,y_train, 
          batch_size=batch_size, 
          epochs=training_epochs, 
          verbose=1, 
          validation_data= (x_test,y_test))

# evaluate the model
score = model.evaluate(x_test,y_test)


print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""## We can train more

"""

model.summary()

# train the model

