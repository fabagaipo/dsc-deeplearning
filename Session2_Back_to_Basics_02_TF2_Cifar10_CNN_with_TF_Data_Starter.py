# -*- coding: utf-8 -*-
"""Copy of Back to Basics 02 - TF2 Cifar10 CNN - with TF Data - Starter.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RVobvScl1wkf9UWYnO6rEUB7Co5RgyAE

## Cifar10 with TF Data

### Lets check for a GPU
"""

# check for a GPU
!nvidia-smi -L

ls

# imports
import matplotlib.pyplot as plt
import numpy as np

import tensorflow as tf

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Flatten, Input
from tensorflow.keras.layers import Conv2D, MaxPooling2D

from tensorflow.keras.layers.experimental import preprocessing

from tensorflow.keras.datasets import cifar10

print(tf.__version__)

# loading the Cifar10 dataset
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

# hyper parameters
batch_size = 64
num_classes = 10
epochs = 12
learning_rate = 0.001

# input image dimensions
img_rows, img_cols = 32,32

"""## Plot some Cifar10"""

# Assign the class names
class_names = ['airplane', 'car', 'bird', 'cat', 'deer',
               'dog', 'frog', 'horse', 'ship', 'truck']

# Plot some images
plt.figure(figsize=(10,10))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.grid(False)
    plt.imshow(x_train[i])

    plt.title(class_names[y_train[i][0]])
    # plt.xlabel(class_names[y_train[i][0]])
plt.show()

"""## Normalizing"""

# create the normalization layer
normalizer = preprocessing.Normalization(name='Norm_layer')
normalizer.adapt(x_train)

# test our norm layer
normalized_data = normalizer(x_train)

# original mean and std
x_train.mean(), x_train.std()

# normalized mean and std
normalized_data.numpy().mean() , normalized_data.numpy().std()

"""## Creating TF Data pipeline"""

y_train[0]

# convert class vectors to binary class matrices
y_train = tf.keras.utils.to_categorical(y_train, num_classes)
y_test = tf.keras.utils.to_categorical(y_test, num_classes)

y_train.shape, y_test.shape

y_train[0]

dataset = tf.data.Dataset.from_tensor_slices((x_train,y_train))
dataset = dataset.shuffle(50000)
dataset = dataset.batch(batch_size, drop_remainder=True)

dataset

# print out shapes for a batch
for images,labels in dataset:
  print(images.shape, labels.shape)
  break

# how many batches in our dataset?
print(len(dataset))

"""## Create our model"""

input_shape = (32,32,3)

input_shape

# create the model

Inp = Input(shape=input_shape,name='Inputs')
x = normalizer(Inp)
x = Conv2D(32, kernel_size=(3,3), activation='relu', name='Conv_01')(x)
x = MaxPooling2D(pool_size=(2,2),name='MaxPooling_01')(x)
x = Conv2D(64, (3,3), activation='relu', name='Conv_02')(x)
x = MaxPooling2D(pool_size=(2,2),name='MaxPooling_02')(x)
x = Conv2D(64, (3,3), activation='relu', name='Conv_03')(x)
x = Flatten(name='Flatten')(x)
x = Dense(64, activation='relu', name='Dense_01')(x)
output = Dense(num_classes, activation='softmax',name='Outputs')(x)

model = Model(Inp,output,name='CNN_Model')

model.summary()

#optimizer
opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)

# compile the model
model.compile(optimizer=opt,
              loss=tf.keras.losses.categorical_crossentropy, #'categorical_crossentropy'
              metrics=['accuracy']
              )

"""## Create a Tensorboard Callback"""

logdir = "./logs/cifar/" 

# Define the basic TensorBoard callback for logging the scalars etc
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)

# train the model

model.fit(dataset, 
          epochs=epochs,
          verbose=1,
          callbacks=[tensorboard_callback],
          steps_per_epoch=len(dataset),
          validation_data=(x_test,y_test))

# evaluate the model
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

"""## TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# Start TensorBoard.

# %tensorboard --logdir=./logs/ --load_fast=false

"""## Make some predictions"""

# make some predictions
preds = model.predict(x_test[:10])

# predictions shape
preds.shape

# look at a single prediction
preds[0]

# convert the labels 
np.argmax(y_test[:10], axis=1)

# convert the predictions
np.argmax(preds, axis=1)

"""## Create a Saved_Model"""

#model path
saved_model_path = "./saved_models/current_model"

#model save
model.save(saved_model_path, save_format='tf')

ls saved_models/current_model/variables/



"""## Homework

Convert this for MNIST
"""

